{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Check the availbility of GPU","metadata":{"id":"sknaEID8BGWd"}},{"cell_type":"code","source":"import tensorflow as tf\nprint(len(tf.config.list_physical_devices('GPU')))\nimport torch\nprint(torch.cuda.is_available())","metadata":{"pycharm":{"name":"#%%\n"},"id":"JoULgUdDj4SA","outputId":"f6b64868-96d0-40a1-a02b-8801b08de5bd","execution":{"iopub.status.busy":"2022-09-20T06:39:54.671714Z","iopub.execute_input":"2022-09-20T06:39:54.672874Z","iopub.status.idle":"2022-09-20T06:40:04.100360Z","shell.execute_reply.started":"2022-09-20T06:39:54.672741Z","shell.execute_reply":"2022-09-20T06:40:04.098987Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2022-09-20 06:40:01.695765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-20 06:40:01.912936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-20 06:40:01.917261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","output_type":"stream"},{"name":"stdout","text":"1\nTrue\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Install gdown library","metadata":{}},{"cell_type":"code","source":"!pip install gdown","metadata":{"execution":{"iopub.status.busy":"2022-09-20T06:40:07.675144Z","iopub.execute_input":"2022-09-20T06:40:07.676921Z","iopub.status.idle":"2022-09-20T06:40:38.281315Z","shell.execute_reply.started":"2022-09-20T06:40:07.676872Z","shell.execute_reply":"2022-09-20T06:40:38.279636Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting gdown\n  Downloading gdown-4.5.1.tar.gz (14 kB)\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.7/site-packages (from gdown) (2.27.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from gdown) (4.64.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from gdown) (3.6.0)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from gdown) (4.11.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gdown) (1.16.0)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->gdown) (2.3.1)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2.0.12)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.26.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2022.6.15)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.7.1)\nBuilding wheels for collected packages: gdown\n  Building wheel for gdown (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gdown: filename=gdown-4.5.1-py3-none-any.whl size=14933 sha256=af09b669cb822466e64ba734a5f23c973e40f1259796cc80a8bdc324b254fc51\n  Stored in directory: /root/.cache/pip/wheels/3d/ec/b0/a96d1d126183f98570a785e6bf8789fca559853a9260e928e1\nSuccessfully built gdown\nInstalling collected packages: gdown\nSuccessfully installed gdown-4.5.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# Download datasets","metadata":{"id":"eLlcFzgHlhFW"}},{"cell_type":"code","source":"!gdown --id 1SRB7w6x_6oVUOzJihlYA5T2VR8u0UJyd\n!gdown --id 1zs91kg3MO6FNkmtHFo1bqOF2Iy1F1b4y\n!gdown --id 165kzfZDsRTZAAfZKedeZiUlKzMcHNgPd","metadata":{"id":"6ID2bvGilgSU","outputId":"608c0304-b030-4db5-80e8-902cb3087b8b","execution":{"iopub.status.busy":"2022-09-20T06:40:38.284938Z","iopub.execute_input":"2022-09-20T06:40:38.285954Z","iopub.status.idle":"2022-09-20T06:40:49.693424Z","shell.execute_reply.started":"2022-09-20T06:40:38.285904Z","shell.execute_reply":"2022-09-20T06:40:49.691825Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.7/site-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  category=FutureWarning,\nDownloading...\nFrom: https://drive.google.com/uc?id=1SRB7w6x_6oVUOzJihlYA5T2VR8u0UJyd\nTo: /kaggle/working/Twitter_train.csv\n100%|████████████████████████████████████████| 261k/261k [00:00<00:00, 58.0MB/s]\n/opt/conda/lib/python3.7/site-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  category=FutureWarning,\nDownloading...\nFrom: https://drive.google.com/uc?id=1zs91kg3MO6FNkmtHFo1bqOF2Iy1F1b4y\nTo: /kaggle/working/Twitter_test.csv\n100%|██████████████████████████████████████| 84.4k/84.4k [00:00<00:00, 43.3MB/s]\n/opt/conda/lib/python3.7/site-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  category=FutureWarning,\nDownloading...\nFrom: https://drive.google.com/uc?id=165kzfZDsRTZAAfZKedeZiUlKzMcHNgPd\nTo: /kaggle/working/Arabic_stop_words.txt\n100%|██████████████████████████████████████| 6.48k/6.48k [00:00<00:00, 7.01MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pyarabic","metadata":{"id":"L77LFKMYkJZg","outputId":"fd410e8f-7233-412f-83ea-82c19f76e8a4","execution":{"iopub.status.busy":"2022-09-20T06:40:57.655565Z","iopub.execute_input":"2022-09-20T06:40:57.656030Z","iopub.status.idle":"2022-09-20T06:41:09.300204Z","shell.execute_reply.started":"2022-09-20T06:40:57.655991Z","shell.execute_reply":"2022-09-20T06:41:09.298576Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyarabic in /opt/conda/lib/python3.7/site-packages (0.6.14)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from pyarabic) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import pyarabic.araby as ar\n\n# import Stemmer\nimport functools, operator\n\nimport logging\n\nlogging.basicConfig(level=logging.WARNING)\nlogger = logging.getLogger(__name__)","metadata":{"pycharm":{"name":"#%%\n"},"id":"xDZ08i40j4SH","execution":{"iopub.status.busy":"2022-09-20T06:41:09.303218Z","iopub.execute_input":"2022-09-20T06:41:09.305034Z","iopub.status.idle":"2022-09-20T06:41:09.329447Z","shell.execute_reply.started":"2022-09-20T06:41:09.304977Z","shell.execute_reply":"2022-09-20T06:41:09.328162Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Load Dataset","metadata":{"pycharm":{"name":"#%% md\n"},"id":"Y-BpP8TZugwK"}},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"./Twitter_train.csv\")\nseed=42","metadata":{"pycharm":{"name":"#%%\n"},"id":"b90o2vElugwT","execution":{"iopub.status.busy":"2022-09-20T06:41:15.535501Z","iopub.execute_input":"2022-09-20T06:41:15.536301Z","iopub.status.idle":"2022-09-20T06:41:15.566536Z","shell.execute_reply.started":"2022-09-20T06:41:15.536266Z","shell.execute_reply":"2022-09-20T06:41:15.565227Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-09-20T06:41:40.928749Z","iopub.execute_input":"2022-09-20T06:41:40.929485Z","iopub.status.idle":"2022-09-20T06:41:40.956069Z","shell.execute_reply.started":"2022-09-20T06:41:40.929438Z","shell.execute_reply":"2022-09-20T06:41:40.954645Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                                  tweet class\n1695  ' \"\"\"@horyagana ان شاء الله ، النصر من عند الل...   pos\n1165                 ' @hameed_farouq هو الكلام اكيد؟ '   neu\n366   ' غزة بين ذكرى حرب و أخرى .. وما زالت .. الحرب...   neg\n1385  ' @NoureenNaguib نمَّ الي علمي انك اتخرجتي وا ...   pos\n887   ' @mahmoud20ma  انا سايبه الموب وفاتحه من لاب ...   neu","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1695</th>\n      <td>' \"\"\"@horyagana ان شاء الله ، النصر من عند الل...</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>1165</th>\n      <td>' @hameed_farouq هو الكلام اكيد؟ '</td>\n      <td>neu</td>\n    </tr>\n    <tr>\n      <th>366</th>\n      <td>' غزة بين ذكرى حرب و أخرى .. وما زالت .. الحرب...</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>1385</th>\n      <td>' @NoureenNaguib نمَّ الي علمي انك اتخرجتي وا ...</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>887</th>\n      <td>' @mahmoud20ma  انا سايبه الموب وفاتحه من لاب ...</td>\n      <td>neu</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Arabic stop words","metadata":{"pycharm":{"name":"#%% md\n"},"id":"Zr_8LPo0ugwX"}},{"cell_type":"code","source":"arabic_stop_words=[]\nwith open ('./Arabic_stop_words.txt',encoding='utf-8') as f :\n    for i in f.readlines() :\n        arabic_stop_words.append(i)\n        arabic_stop_words[-1]=arabic_stop_words[-1][:-1]\n","metadata":{"pycharm":{"name":"#%%\n"},"id":"lJ0o2qs7ugwZ","execution":{"iopub.status.busy":"2022-09-20T06:41:43.510811Z","iopub.execute_input":"2022-09-20T06:41:43.511280Z","iopub.status.idle":"2022-09-20T06:41:43.520453Z","shell.execute_reply.started":"2022-09-20T06:41:43.511237Z","shell.execute_reply":"2022-09-20T06:41:43.518770Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"!pip install farasapy","metadata":{"pycharm":{"name":"#%%\n"},"id":"ETdZlcCHj4SM","outputId":"daac8d2c-3063-4909-e738-cc660bf6a67b","execution":{"iopub.status.busy":"2022-09-20T06:41:45.991213Z","iopub.execute_input":"2022-09-20T06:41:45.992074Z","iopub.status.idle":"2022-09-20T06:41:59.249671Z","shell.execute_reply.started":"2022-09-20T06:41:45.992024Z","shell.execute_reply":"2022-09-20T06:41:59.247663Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Collecting farasapy\n  Downloading farasapy-0.0.14-py3-none-any.whl (11 kB)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from farasapy) (2.27.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from farasapy) (4.64.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->farasapy) (2022.6.15)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->farasapy) (2.0.12)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->farasapy) (1.26.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->farasapy) (3.3)\nInstalling collected packages: farasapy\nSuccessfully installed farasapy-0.0.14\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\n\n#============= Read CSV and apply data preperation =============#\n\n\ndef data_preprocessing (data_frame):\n    # clean-up: remove #tags, http links and special symbols\n    data_frame['tweet']= data_frame['tweet'].apply(lambda x: x[2:-2])\n    data_frame['tweet']= data_frame['tweet'].apply(lambda x: re.sub(r'http\\S+', '', x))\n    data_frame['tweet'] = data_frame['tweet'].apply(lambda x: re.sub(r'[@|#]\\S*', '', x))\n    data_frame['tweet'] = data_frame['tweet'].apply(lambda x: re.sub(r'\"+', '', x))\n\n    # Remove arabic signs\n    data_frame['tweet'] = data_frame['tweet'].apply(lambda x: re.sub(r'([@A-Za-z0-9_ـــــــــــــ]+)|[^\\w\\s]|#|http\\S+', '', x))\n\n    # Remove repeated letters like \"الللللللللللللللله\" to \"الله\"\n    data_frame['tweet'] = data_frame['tweet'].apply(lambda x: x[0:2] + ''.join([x[i] for i in range(2, len(x)) if x[i]!=x[i-1] or x[i]!=x[i-2]]))\n\n    # remove stop words\n    data_frame['tweet'] = data_frame['tweet'].apply(lambda x: '' if x in arabic_stop_words else x)\n\n    from nltk.stem.isri import ISRIStemmer\n    df['tweet']=df['tweet'].apply(lambda x:ISRIStemmer().stem(x))\n\n    return data_frame\n","metadata":{"pycharm":{"name":"#%%\n"},"id":"O7VPzTXYugwo","execution":{"iopub.status.busy":"2022-09-20T06:42:00.821420Z","iopub.execute_input":"2022-09-20T06:42:00.821898Z","iopub.status.idle":"2022-09-20T06:42:00.837702Z","shell.execute_reply.started":"2022-09-20T06:42:00.821860Z","shell.execute_reply":"2022-09-20T06:42:00.836049Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"!pip install emoji","metadata":{"id":"zOe29KXMmh1B","outputId":"394b9a69-af09-4817-90d3-d885b253ada8","execution":{"iopub.status.busy":"2022-09-20T06:42:02.782850Z","iopub.execute_input":"2022-09-20T06:42:02.783668Z","iopub.status.idle":"2022-09-20T06:42:14.413204Z","shell.execute_reply.started":"2022-09-20T06:42:02.783633Z","shell.execute_reply":"2022-09-20T06:42:14.411309Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Requirement already satisfied: emoji in /opt/conda/lib/python3.7/site-packages (1.7.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# st =  Stemmer.Stemmer('arabic')\nimport string,emoji\ndef data_cleaning (text):\n    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n    text = re.sub(r'^http?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n    text = re.sub(r\"http\\S+\", \"\", text)\n    text = re.sub(r\"https\\S+\", \"\", text)\n    text = re.sub(r'\\s+', ' ', text)\n    text = re.sub(\"(\\s\\d+)\",\"\",text)\n    text = re.sub(r\"$\\d+\\W+|\\b\\d+\\b|\\W+\\d+$\", \"\", text)\n    text = re.sub(\"\\d+\", \" \", text)\n    text = ar.strip_tashkeel(text)\n    text = ar.strip_tatweel(text)\n    text = text.replace(\"#\", \" \");\n    text = text.replace(\"@\", \" \");\n    text = text.replace(\"_\", \" \");\n    translator = str.maketrans('', '', string.punctuation)\n    text = text.translate(translator)\n    em = text\n    em_split_emoji = emoji.get_emoji_regexp().split(em)\n    em_split_whitespace = [substr.split() for substr in em_split_emoji]\n    em_split = functools.reduce(operator.concat, em_split_whitespace)\n    text = \" \".join(em_split)\n    text = re.sub(r'(.)\\1+', r'\\1', text)\n    # text_stem = \" \".join([st.stemWord(i) for i in text.split()])\n    # text = text +\" \"+ text_stem\n    text = text.replace(\"آ\", \"ا\")\n    text = text.replace(\"إ\", \"ا\")\n    text = text.replace(\"أ\", \"ا\")\n    text = text.replace(\"ؤ\", \"و\")\n    text = text.replace(\"ئ\", \"ي\")\n\n    return text","metadata":{"pycharm":{"name":"#%%\n"},"id":"WSAZBzasj4SO","execution":{"iopub.status.busy":"2022-09-20T06:42:14.416282Z","iopub.execute_input":"2022-09-20T06:42:14.416783Z","iopub.status.idle":"2022-09-20T06:42:14.473713Z","shell.execute_reply.started":"2022-09-20T06:42:14.416732Z","shell.execute_reply":"2022-09-20T06:42:14.472255Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df['tweet']=df['tweet'].apply(lambda x: data_cleaning(x))\ndf=data_preprocessing(df)\ndf","metadata":{"id":"v_gDzrkBwMz6","outputId":"35dc1e8a-bd77-4b7a-9501-c2ca97a0ed67","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-09-20T06:42:18.358564Z","iopub.execute_input":"2022-09-20T06:42:18.359827Z","iopub.status.idle":"2022-09-20T06:42:20.254896Z","shell.execute_reply.started":"2022-09-20T06:42:18.359784Z","shell.execute_reply":"2022-09-20T06:42:20.253455Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:20: DeprecationWarning: 'emoji.get_emoji_regexp()' is deprecated and will be removed in version 2.0.0. If you want to remove emoji from a string, consider the method emoji.replace_emoji(str, replace='').\nTo hide this warning, pin/downgrade the package to 'emoji~=1.6.3'\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                                                  tweet class\n0     متني الحياه ان الذين يعيشون على الارض ليسوا مل...   pos\n1                             ري كرسمس كل سنة وانتم طيب   pos\n2                                       انتهى مشوار الخ   neg\n3                                  عارف ابتدى مذاكره من   neg\n4      اختصروا الطريق بدلا من اختيار المنصف ثم الانق...   neg\n...                                                 ...   ...\n2054    الجمال مبيحتاح اي مكياج لناعم وله خشن جمل ال...   neu\n2055   نتمني وجود الفنانة رنا سماحة افضل فنانة صاعدة...   neu\n2056  د الهدى فالكاينات ضياء وفم الزمان تبسم وسناء ك...   pos\n2057                               انت متناقض جدا يا صل   neg\n2058         طقة السيدة زينب ليلة المولد مسجد السيدة زي   neu\n\n[2059 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>متني الحياه ان الذين يعيشون على الارض ليسوا مل...</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ري كرسمس كل سنة وانتم طيب</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>انتهى مشوار الخ</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>عارف ابتدى مذاكره من</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>اختصروا الطريق بدلا من اختيار المنصف ثم الانق...</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2054</th>\n      <td>الجمال مبيحتاح اي مكياج لناعم وله خشن جمل ال...</td>\n      <td>neu</td>\n    </tr>\n    <tr>\n      <th>2055</th>\n      <td>نتمني وجود الفنانة رنا سماحة افضل فنانة صاعدة...</td>\n      <td>neu</td>\n    </tr>\n    <tr>\n      <th>2056</th>\n      <td>د الهدى فالكاينات ضياء وفم الزمان تبسم وسناء ك...</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>2057</th>\n      <td>انت متناقض جدا يا صل</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>2058</th>\n      <td>طقة السيدة زينب ليلة المولد مسجد السيدة زي</td>\n      <td>neu</td>\n    </tr>\n  </tbody>\n</table>\n<p>2059 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"!git clone https://github.com/aub-mind/arabert.git","metadata":{"pycharm":{"name":"#%%\n"},"id":"MyaQC-Axj4SQ","outputId":"42c6c351-0166-4a30-ee3c-e91c49e8bb4b","execution":{"iopub.status.busy":"2022-09-20T06:42:29.902589Z","iopub.execute_input":"2022-09-20T06:42:29.903021Z","iopub.status.idle":"2022-09-20T06:42:33.081616Z","shell.execute_reply.started":"2022-09-20T06:42:29.902991Z","shell.execute_reply":"2022-09-20T06:42:33.080048Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Cloning into 'arabert'...\nremote: Enumerating objects: 595, done.\u001b[K\nremote: Counting objects: 100% (60/60), done.\u001b[K\nremote: Compressing objects: 100% (29/29), done.\u001b[K\nremote: Total 595 (delta 37), reused 43 (delta 29), pack-reused 535\u001b[K\nReceiving objects: 100% (595/595), 9.14 MiB | 11.65 MiB/s, done.\nResolving deltas: 100% (338/338), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"from arabert.preprocess import ArabertPreprocessor\n\nmodel_name = \"aubmindlab/bert-large-arabertv02-twitter\"\narabert_prep = ArabertPreprocessor(model_name=model_name)\n\ndf['tweet']=df['tweet'].apply(lambda x: arabert_prep.preprocess(x))\n\n\ntext = \"ولن نبالغ إذا قلنا: إن 'هاتف' أو 'كمبيوتر المكتب' في زمننا هذا ضروري\"\narabert_prep.preprocess(text)\n# # \"و+ لن نبالغ إذا قل +نا : إن ' هاتف ' أو ' كمبيوتر ال+ مكتب ' في زمن +نا هذا ضروري\"","metadata":{"pycharm":{"name":"#%%\n"},"id":"omMDhuNzj4SR","execution":{"iopub.status.busy":"2022-09-20T06:42:54.696221Z","iopub.execute_input":"2022-09-20T06:42:54.697322Z","iopub.status.idle":"2022-09-20T06:42:54.926583Z","shell.execute_reply.started":"2022-09-20T06:42:54.697275Z","shell.execute_reply":"2022-09-20T06:42:54.925136Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"\"ولن نبالغ إذا قلنا : إن ' هاتف ' أو ' كمبيوتر المكتب ' في زمننا هذا ضروري\""},"metadata":{}}]},{"cell_type":"markdown","source":"## Label Encoder","metadata":{"pycharm":{"name":"#%% md\n"},"id":"tBEVTgBRugws"}},{"cell_type":"code","source":"from sklearn import preprocessing\n# Apply label encoding over the labels\nlable_encoder = preprocessing.LabelEncoder()\nencoded_labels =lable_encoder.fit_transform(df[\"class\"])\ndf['class']=encoded_labels\ndf","metadata":{"pycharm":{"name":"#%%\n"},"id":"nvWAmR5nugwu","outputId":"ebe05a07-30d2-47f5-9cfa-a6ffdbcaae1f","execution":{"iopub.status.busy":"2022-09-20T06:43:19.425551Z","iopub.execute_input":"2022-09-20T06:43:19.426160Z","iopub.status.idle":"2022-09-20T06:43:19.452953Z","shell.execute_reply.started":"2022-09-20T06:43:19.426114Z","shell.execute_reply":"2022-09-20T06:43:19.451623Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"                                                  tweet  class\n0     متني الحياه ان الذين يعيشون على الارض ليسوا مل...      2\n1                             ري كرسمس كل سنة وانتم طيب      2\n2                                       انتهى مشوار الخ      0\n3                                  عارف ابتدى مذاكره من      0\n4     اختصروا الطريق بدلا من اختيار المنصف ثم الانقل...      0\n...                                                 ...    ...\n2054  الجمال مبيحتاح اي مكياج لناعم وله خشن جمل الطا...      1\n2055  نتمني وجود الفنانة رنا سماحة افضل فنانة صاعدة ...      1\n2056  د الهدى فالكاينات ضياء وفم الزمان تبسم وسناء ك...      2\n2057                               انت متناقض جدا يا صل      0\n2058         طقة السيدة زينب ليلة المولد مسجد السيدة زي      1\n\n[2059 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>متني الحياه ان الذين يعيشون على الارض ليسوا مل...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ري كرسمس كل سنة وانتم طيب</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>انتهى مشوار الخ</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>عارف ابتدى مذاكره من</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>اختصروا الطريق بدلا من اختيار المنصف ثم الانقل...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2054</th>\n      <td>الجمال مبيحتاح اي مكياج لناعم وله خشن جمل الطا...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2055</th>\n      <td>نتمني وجود الفنانة رنا سماحة افضل فنانة صاعدة ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2056</th>\n      <td>د الهدى فالكاينات ضياء وفم الزمان تبسم وسناء ك...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2057</th>\n      <td>انت متناقض جدا يا صل</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2058</th>\n      <td>طقة السيدة زينب ليلة المولد مسجد السيدة زي</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>2059 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df['length']=df['tweet'].apply(lambda x:len(x.split(' ')))\ndf['length'].max()","metadata":{"pycharm":{"name":"#%%\n"},"id":"3RgOhu_aj4SS","outputId":"df3da832-335c-4f17-a942-6820bcd6b3ac","execution":{"iopub.status.busy":"2022-09-20T06:43:21.814937Z","iopub.execute_input":"2022-09-20T06:43:21.815335Z","iopub.status.idle":"2022-09-20T06:43:21.831847Z","shell.execute_reply.started":"2022-09-20T06:43:21.815305Z","shell.execute_reply":"2022-09-20T06:43:21.830253Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"36"},"metadata":{}}]},{"cell_type":"markdown","source":"## Train Test Split","metadata":{"pycharm":{"name":"#%% md\n"},"id":"RscS77J9ugww"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_validation, y_train, y_validation=train_test_split(df['tweet'], df['class'], test_size=0.2, random_state=seed)\nX_validation","metadata":{"pycharm":{"name":"#%%\n"},"id":"qhLxzIXvugwx","outputId":"1085ba06-8206-451f-81ea-9aec30b6d802","execution":{"iopub.status.busy":"2022-09-20T06:43:27.720802Z","iopub.execute_input":"2022-09-20T06:43:27.722073Z","iopub.status.idle":"2022-09-20T06:43:27.742190Z","shell.execute_reply.started":"2022-09-20T06:43:27.722016Z","shell.execute_reply":"2022-09-20T06:43:27.740876Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"1298    معناها مش المعني الظاهر معناها نفسها في الجنة ...\n591                                                     ا\n1318                                              كل اخير\n1067                    هنا العاصمة لميس الحديدي تودع عبر\n29                              انا نهي سنفورة القهوة ادي\n                              ...                        \n1033    صباح اورد من احمد انا بحب سكس انا بحب الزمالك ...\n674     حياة بالقرب من اله حياة مطمينة محفوفة بالتوفيق...\n1771                           عليش نسال فيكن معناها ي بص\n322                                         شارع الجاردنز\n1299                              ن نفسي اتولد مخلص ه تعل\nName: tweet, Length: 412, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"# Trying some machine learning models","metadata":{"id":"0cs2vHnqBaXF"}},{"cell_type":"markdown","source":"## TF_IDF","metadata":{"pycharm":{"name":"#%% md\n"},"id":"kevPnoxdugwy"}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ndef tfidf_ngram(n_gram,X_train,X_val):\n    vectorizer = TfidfVectorizer(ngram_range=(n_gram,n_gram))\n    x_train_vec = vectorizer.fit_transform(X_train)\n    x_test_vec = vectorizer.transform(X_val)\n    return x_train_vec,x_test_vec","metadata":{"pycharm":{"name":"#%%\n"},"id":"roXYHxrdugwy","execution":{"iopub.status.busy":"2022-09-20T06:43:35.456206Z","iopub.execute_input":"2022-09-20T06:43:35.456962Z","iopub.status.idle":"2022-09-20T06:43:35.464055Z","shell.execute_reply.started":"2022-09-20T06:43:35.456914Z","shell.execute_reply":"2022-09-20T06:43:35.462164Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Applying tfidf with 1-gram, 2-gram and 3-gram\ntfidf_1g_transformation_train,tfidf_1g_transformation_validation= tfidf_ngram(1,X_train,X_validation)\ntfidf_2g_transformation_train,tfidf_2g_transformation_validation= tfidf_ngram(2,X_train,X_validation)","metadata":{"pycharm":{"name":"#%%\n"},"id":"iuWEPExnugwz","execution":{"iopub.status.busy":"2022-09-20T06:43:37.960592Z","iopub.execute_input":"2022-09-20T06:43:37.961485Z","iopub.status.idle":"2022-09-20T06:43:38.077389Z","shell.execute_reply.started":"2022-09-20T06:43:37.961439Z","shell.execute_reply":"2022-09-20T06:43:38.075906Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Machine learning models","metadata":{"pycharm":{"name":"#%% md\n"},"id":"7BYrD0RLugw0"}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nmodels=[SVC(),XGBClassifier(),RandomForestClassifier(),DecisionTreeClassifier(),LogisticRegression()]\nfor m in models :\n    m.fit(tfidf_2g_transformation_train,y_train)\n    print(m.score(tfidf_2g_transformation_train,y_train))\n    print(m.score(tfidf_2g_transformation_validation,y_validation))","metadata":{"pycharm":{"name":"#%%\n"},"id":"RCB9CYrwugw0","outputId":"47b8a203-633d-4f83-c343-2fe875fe6495","execution":{"iopub.status.busy":"2022-09-20T06:43:43.764196Z","iopub.execute_input":"2022-09-20T06:43:43.766152Z","iopub.status.idle":"2022-09-20T06:43:55.053887Z","shell.execute_reply.started":"2022-09-20T06:43:43.766105Z","shell.execute_reply":"2022-09-20T06:43:55.049410Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"0.9769277474195507\n0.3422330097087379\n0.4632665452337584\n0.3446601941747573\n0.9775349119611415\n0.35436893203883496\n0.9775349119611415\n0.35436893203883496\n0.9769277474195507\n0.36650485436893204\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Trying to use some pre-trained models from hugging face website ","metadata":{"id":"EnB7E1uNBifr"}},{"cell_type":"markdown","source":"## Install transformers","metadata":{"pycharm":{"name":"#%% md\n"},"id":"546gZdstugw1"}},{"cell_type":"code","source":"!pip install transformers","metadata":{"pycharm":{"name":"#%%\n"},"id":"1PPa1X-Tugw1","outputId":"82c1ef71-0651-41ea-8c26-0747295232bd","execution":{"iopub.status.busy":"2022-09-20T06:44:04.127996Z","iopub.execute_input":"2022-09-20T06:44:04.128439Z","iopub.status.idle":"2022-09-20T06:44:15.451540Z","shell.execute_reply.started":"2022-09-20T06:44:04.128400Z","shell.execute_reply":"2022-09-20T06:44:15.450029Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.18.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.7.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.6.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.53)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.27.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.9)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.6.15)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.4)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"## Model and Tokenizer initialization","metadata":{"id":"Vn5ecTRICc_J"}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n#============= Initialize Arabic Bert =============#\n#load your pre_trained model with all its weights\n# model_name= 'aubmindlab/bert-base-arabertv02'\nmodel_name='UBC-NLP/MARBERT' #top\n# model_name='asafaya/bert-base-arabic'\n# model_name='AraBERTv0.2-Twitter-base'\n# model_name='aubmindlab/bert-large-arabertv2'\n# model_name='aubmindlab/bert-base-arabertv02-twitter'\n# model_name='aubmindlab/bert-large-arabertv02-twitter'\n# model_name='aubmindlab/aragpt2-base'\n\n# model_name='aubmindlab/bert-base-arabertv2'\ntokenizer =AutoTokenizer.from_pretrained(model_name)\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n# model=AutoModel.from_pretrained(model_name,output_hidden_states=True)","metadata":{"pycharm":{"name":"#%%\n"},"id":"WqHEVmquugw1","outputId":"b85a3f25-9dd1-40c6-b273-56de73263710","execution":{"iopub.status.busy":"2022-09-20T06:44:15.455474Z","iopub.execute_input":"2022-09-20T06:44:15.456073Z","iopub.status.idle":"2022-09-20T06:44:50.060072Z","shell.execute_reply.started":"2022-09-20T06:44:15.455989Z","shell.execute_reply":"2022-09-20T06:44:50.058885Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/376 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"856d4baacbbb42d28202c9cb07c13255"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/701 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"351255967dfa43c490df3514bbf9291e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.05M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32a45a19207e4b72b28eb30c3d3565c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94b62f7b27a44588865cc3e08a0e0993"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/624M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30d398b8d36d4935b197a787adf32db2"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at UBC-NLP/MARBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at UBC-NLP/MARBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"You can uncomment ay of the other models to get differnet accuraces","metadata":{"id":"sj6MbwJaB-iz"}},{"cell_type":"code","source":"# Tokenize the sentences using bert tokenizer\ndf[\"bert_tokens\"] = df.tweet.apply(lambda x: tokenizer(x).tokens())\ndf[\"bert_tokens_ids\"] = df.tweet.apply(lambda x: tokenizer(x).tokens())\ndf[\"encoded\"] = df.tweet.apply(lambda x: tokenizer.encode_plus(x,return_tensors='pt')['input_ids'])\ndf","metadata":{"pycharm":{"name":"#%%\n"},"id":"sc5VRJOCugw2","outputId":"d500a758-62b9-4dec-8047-b19c7517d499","execution":{"iopub.status.busy":"2022-09-20T06:45:27.776690Z","iopub.execute_input":"2022-09-20T06:45:27.777157Z","iopub.status.idle":"2022-09-20T06:45:28.927896Z","shell.execute_reply.started":"2022-09-20T06:45:27.777125Z","shell.execute_reply":"2022-09-20T06:45:28.926496Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"                                                  tweet  class  length  \\\n0     متني الحياه ان الذين يعيشون على الارض ليسوا مل...      2      27   \n1                             ري كرسمس كل سنة وانتم طيب      2       6   \n2                                       انتهى مشوار الخ      0       3   \n3                                  عارف ابتدى مذاكره من      0       4   \n4     اختصروا الطريق بدلا من اختيار المنصف ثم الانقل...      0      20   \n...                                                 ...    ...     ...   \n2054  الجمال مبيحتاح اي مكياج لناعم وله خشن جمل الطا...      1      10   \n2055  نتمني وجود الفنانة رنا سماحة افضل فنانة صاعدة ...      1      11   \n2056  د الهدى فالكاينات ضياء وفم الزمان تبسم وسناء ك...      2      16   \n2057                               انت متناقض جدا يا صل      0       5   \n2058         طقة السيدة زينب ليلة المولد مسجد السيدة زي      1       8   \n\n                                            bert_tokens  \\\n0     [[CLS], متني, الحياه, ان, الذين, يعيشون, على, ...   \n1     [[CLS], ري, كرس, ##مس, كل, سنة, وانتم, طيب, [S...   \n2                     [[CLS], انتهى, مشوار, الخ, [SEP]]   \n3               [[CLS], عارف, ابتدى, مذاكره, من, [SEP]]   \n4     [[CLS], اختصر, ##وا, الطريق, بدلا, من, اختيار,...   \n...                                                 ...   \n2054  [[CLS], الجمال, مبيح, ##تاح, اي, مكياج, لنا, #...   \n2055  [[CLS], نتمني, وجود, الفنانة, رنا, سماحة, افضل...   \n2056  [[CLS], د, الهدى, فالك, ##اينات, ضياء, وف, ##م...   \n2057           [[CLS], انت, متناقض, جدا, يا, صل, [SEP]]   \n2058  [[CLS], طق, ##ة, السيدة, زينب, ليلة, المولد, م...   \n\n                                        bert_tokens_ids  \\\n0     [[CLS], متني, الحياه, ان, الذين, يعيشون, على, ...   \n1     [[CLS], ري, كرس, ##مس, كل, سنة, وانتم, طيب, [S...   \n2                     [[CLS], انتهى, مشوار, الخ, [SEP]]   \n3               [[CLS], عارف, ابتدى, مذاكره, من, [SEP]]   \n4     [[CLS], اختصر, ##وا, الطريق, بدلا, من, اختيار,...   \n...                                                 ...   \n2054  [[CLS], الجمال, مبيح, ##تاح, اي, مكياج, لنا, #...   \n2055  [[CLS], نتمني, وجود, الفنانة, رنا, سماحة, افضل...   \n2056  [[CLS], د, الهدى, فالك, ##اينات, ضياء, وف, ##م...   \n2057           [[CLS], انت, متناقض, جدا, يا, صل, [SEP]]   \n2058  [[CLS], طق, ##ة, السيدة, زينب, ليلة, المولد, م...   \n\n                                                encoded  \n0     [[tensor(2), tensor(68713), tensor(3946), tens...  \n1     [[tensor(2), tensor(2536), tensor(35685), tens...  \n2     [[tensor(2), tensor(7609), tensor(13606), tens...  \n3     [[tensor(2), tensor(3323), tensor(45008), tens...  \n4     [[tensor(2), tensor(22181), tensor(1958), tens...  \n...                                                 ...  \n2054  [[tensor(2), tensor(4770), tensor(68899), tens...  \n2055  [[tensor(2), tensor(39939), tensor(3715), tens...  \n2056  [[tensor(2), tensor(125), tensor(4880), tensor...  \n2057  [[tensor(2), tensor(2030), tensor(27008), tens...  \n2058  [[tensor(2), tensor(16252), tensor(1046), tens...  \n\n[2059 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>class</th>\n      <th>length</th>\n      <th>bert_tokens</th>\n      <th>bert_tokens_ids</th>\n      <th>encoded</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>متني الحياه ان الذين يعيشون على الارض ليسوا مل...</td>\n      <td>2</td>\n      <td>27</td>\n      <td>[[CLS], متني, الحياه, ان, الذين, يعيشون, على, ...</td>\n      <td>[[CLS], متني, الحياه, ان, الذين, يعيشون, على, ...</td>\n      <td>[[tensor(2), tensor(68713), tensor(3946), tens...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ري كرسمس كل سنة وانتم طيب</td>\n      <td>2</td>\n      <td>6</td>\n      <td>[[CLS], ري, كرس, ##مس, كل, سنة, وانتم, طيب, [S...</td>\n      <td>[[CLS], ري, كرس, ##مس, كل, سنة, وانتم, طيب, [S...</td>\n      <td>[[tensor(2), tensor(2536), tensor(35685), tens...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>انتهى مشوار الخ</td>\n      <td>0</td>\n      <td>3</td>\n      <td>[[CLS], انتهى, مشوار, الخ, [SEP]]</td>\n      <td>[[CLS], انتهى, مشوار, الخ, [SEP]]</td>\n      <td>[[tensor(2), tensor(7609), tensor(13606), tens...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>عارف ابتدى مذاكره من</td>\n      <td>0</td>\n      <td>4</td>\n      <td>[[CLS], عارف, ابتدى, مذاكره, من, [SEP]]</td>\n      <td>[[CLS], عارف, ابتدى, مذاكره, من, [SEP]]</td>\n      <td>[[tensor(2), tensor(3323), tensor(45008), tens...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>اختصروا الطريق بدلا من اختيار المنصف ثم الانقل...</td>\n      <td>0</td>\n      <td>20</td>\n      <td>[[CLS], اختصر, ##وا, الطريق, بدلا, من, اختيار,...</td>\n      <td>[[CLS], اختصر, ##وا, الطريق, بدلا, من, اختيار,...</td>\n      <td>[[tensor(2), tensor(22181), tensor(1958), tens...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2054</th>\n      <td>الجمال مبيحتاح اي مكياج لناعم وله خشن جمل الطا...</td>\n      <td>1</td>\n      <td>10</td>\n      <td>[[CLS], الجمال, مبيح, ##تاح, اي, مكياج, لنا, #...</td>\n      <td>[[CLS], الجمال, مبيح, ##تاح, اي, مكياج, لنا, #...</td>\n      <td>[[tensor(2), tensor(4770), tensor(68899), tens...</td>\n    </tr>\n    <tr>\n      <th>2055</th>\n      <td>نتمني وجود الفنانة رنا سماحة افضل فنانة صاعدة ...</td>\n      <td>1</td>\n      <td>11</td>\n      <td>[[CLS], نتمني, وجود, الفنانة, رنا, سماحة, افضل...</td>\n      <td>[[CLS], نتمني, وجود, الفنانة, رنا, سماحة, افضل...</td>\n      <td>[[tensor(2), tensor(39939), tensor(3715), tens...</td>\n    </tr>\n    <tr>\n      <th>2056</th>\n      <td>د الهدى فالكاينات ضياء وفم الزمان تبسم وسناء ك...</td>\n      <td>2</td>\n      <td>16</td>\n      <td>[[CLS], د, الهدى, فالك, ##اينات, ضياء, وف, ##م...</td>\n      <td>[[CLS], د, الهدى, فالك, ##اينات, ضياء, وف, ##م...</td>\n      <td>[[tensor(2), tensor(125), tensor(4880), tensor...</td>\n    </tr>\n    <tr>\n      <th>2057</th>\n      <td>انت متناقض جدا يا صل</td>\n      <td>0</td>\n      <td>5</td>\n      <td>[[CLS], انت, متناقض, جدا, يا, صل, [SEP]]</td>\n      <td>[[CLS], انت, متناقض, جدا, يا, صل, [SEP]]</td>\n      <td>[[tensor(2), tensor(2030), tensor(27008), tens...</td>\n    </tr>\n    <tr>\n      <th>2058</th>\n      <td>طقة السيدة زينب ليلة المولد مسجد السيدة زي</td>\n      <td>1</td>\n      <td>8</td>\n      <td>[[CLS], طق, ##ة, السيدة, زينب, ليلة, المولد, م...</td>\n      <td>[[CLS], طق, ##ة, السيدة, زينب, ليلة, المولد, م...</td>\n      <td>[[tensor(2), tensor(16252), tensor(1046), tens...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2059 rows × 6 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Padding and attention mask","metadata":{"pycharm":{"name":"#%% md\n"},"id":"fhb0-jQMugw4"}},{"cell_type":"code","source":"from keras_preprocessing.sequence import pad_sequences\n\n# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway.\n# In the original paper, the authors used a length of 512.\nMAX_LEN = 64\n# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\ninput_ids = [tokenizer.convert_tokens_to_ids(x) for x in df['bert_tokens']]\n# Pad our input tokens\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n# Create attention masks\nattention_masks = []\n\n# Create a mask of 1s for each token followed by 0s for padding\nfor seq in input_ids:\n    seq_mask = [float(i>0) for i in seq]\n    attention_masks.append(seq_mask)","metadata":{"pycharm":{"name":"#%%\n"},"id":"taCZLE63ugw4","execution":{"iopub.status.busy":"2022-09-20T06:45:59.429824Z","iopub.execute_input":"2022-09-20T06:45:59.430247Z","iopub.status.idle":"2022-09-20T06:45:59.544057Z","shell.execute_reply.started":"2022-09-20T06:45:59.430215Z","shell.execute_reply":"2022-09-20T06:45:59.542614Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\n# Use train_test_split to split our data into train and validation sets for training\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, encoded_labels,\n                                                            random_state=seed, test_size=0.1)\ntrain_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n                                             random_state=seed, test_size=0.1)\n# Convert all of our data into torch tensors, the required datatype for our model\n\ntrain_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\ntrain_labels = torch.tensor(train_labels)\nvalidation_labels = torch.tensor(validation_labels)\ntrain_masks = torch.tensor(train_masks)\nvalidation_masks = torch.tensor(validation_masks)\n# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\nbatch_size = 64\n\n# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop,\n# with an iterator the entire dataset does not need to be loaded into memory\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_dataloader = DataLoader(train_data, batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_dataloader = DataLoader(validation_data, batch_size=batch_size)","metadata":{"pycharm":{"name":"#%%\n"},"id":"ip_wd-t9ugw5","execution":{"iopub.status.busy":"2022-09-20T06:46:01.813916Z","iopub.execute_input":"2022-09-20T06:46:01.814408Z","iopub.status.idle":"2022-09-20T06:46:01.842752Z","shell.execute_reply.started":"2022-09-20T06:46:01.814348Z","shell.execute_reply":"2022-09-20T06:46:01.841422Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## Set optimizer parameters","metadata":{"pycharm":{"name":"#%% md\n"},"id":"CEvllsdYugw5"}},{"cell_type":"code","source":"import torch.optim as optim\n\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'gamma', 'beta']\noptimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],'weight_decay_rate': 0.01},\n                                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],'weight_decay_rate': 0.0}]\n# This variable contains all of the hyperparemeter information our training loop needs\n# optimizer = optim.BertAdam(optimizer_grouped_parameters,lr=2e-5,warmup=.1)\n# optimizer = optim.AdamW(optimizer_grouped_parameters,lr=5e-6)\noptimizer = optim.AdamW(optimizer_grouped_parameters,lr=.00001)","metadata":{"pycharm":{"name":"#%%\n"},"id":"SIutfqrnugw5","execution":{"iopub.status.busy":"2022-09-20T06:46:05.122867Z","iopub.execute_input":"2022-09-20T06:46:05.123273Z","iopub.status.idle":"2022-09-20T06:46:05.135476Z","shell.execute_reply.started":"2022-09-20T06:46:05.123241Z","shell.execute_reply":"2022-09-20T06:46:05.133627Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"pycharm":{"name":"#%% md\n"},"id":"sZZCYaUKugw5"}},{"cell_type":"code","source":"from tqdm import trange\nimport numpy as np\n# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\nt = []\n\n# Store our loss and accuracy for plotting\ntrain_loss_set = []\n\n# Number of training epochs\nepochs = 11\n\n# Transfer the model to GPU\nmodel.to(\"cuda\")\n\n# trange is a tqdm wrapper around the normal python range\nfor _ in trange(epochs, desc=\"Epoch\"):\n\n\n  # Training\n\n  # Set our model to training mode (as opposed to evaluation mode)\n  model.train()\n\n  # Tracking variables\n  tr_loss = 0\n  nb_tr_examples, nb_tr_steps = 0, 0\n\n  # Train the data for one epoch\n  for step, batch in enumerate(train_dataloader):\n    # Add batch to GPU\n    b_input_ids, b_input_mask, b_labels = batch\n    b_labels = b_labels.type(torch.LongTensor)   # casting to long\n    # Clear out the gradients (by default they accumulate)\n    optimizer.zero_grad()\n\n    # Forward pass\n    loss = model(b_input_ids.to(\"cuda\"), token_type_ids=None, attention_mask=b_input_mask.to(\"cuda\"), labels=b_labels.to(\"cuda\"))[\"loss\"]\n    train_loss_set.append(loss.item())\n\n    # Backward pass\n    loss.backward()\n\n    # Update parameters and take a step using the computed gradient\n    optimizer.step()\n\n\n    # Update tracking variables\n    tr_loss += loss.item()\n    nb_tr_examples += b_input_ids.size(0)\n    nb_tr_steps += 1\n\n  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n\n  # Validation\n\n  # Put model in evaluation mode to evaluate loss on the validation set\n  model.eval()\n\n  # Tracking variables\n  eval_loss, eval_accuracy = 0, 0\n  nb_eval_steps, nb_eval_examples = 0, 0\n\n  # Evaluate data for one epoch\n  for batch in validation_dataloader:\n    # Add batch to GPU\n    # batch = tuple(t.to(device) for t in batch)\n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels = batch\n    b_labels = b_labels.type(torch.LongTensor)   # casting to long\n    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n    with torch.no_grad():\n      # Forward pass, calculate logit predictions\n      logits = model(b_input_ids.to(\"cuda\"), token_type_ids=None, attention_mask=b_input_mask.to(\"cuda\"))\n\n    # Move logits and labels to CPU\n    logits = logits[\"logits\"].detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n\n    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n\n    eval_accuracy += tmp_eval_accuracy\n    nb_eval_steps += 1\n\n  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n  if (eval_accuracy/nb_eval_steps) > 0.77 :\n    break","metadata":{"pycharm":{"name":"#%%\n"},"id":"0kfOowbxugw6","outputId":"5f51f984-57ff-485b-f6ef-40b4e3b175c8","execution":{"iopub.status.busy":"2022-09-20T06:48:26.503053Z","iopub.execute_input":"2022-09-20T06:48:26.503490Z","iopub.status.idle":"2022-09-20T06:50:29.549492Z","shell.execute_reply.started":"2022-09-20T06:48:26.503456Z","shell.execute_reply":"2022-09-20T06:50:29.548071Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"Epoch:   0%|          | 0/11 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.146267749626061\n","output_type":"stream"},{"name":"stderr","text":"Epoch:   9%|▉         | 1/11 [00:11<01:51, 11.14s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.7472098214285714\nTrain loss: 0.11006446529565186\n","output_type":"stream"},{"name":"stderr","text":"Epoch:  18%|█▊        | 2/11 [00:22<01:40, 11.18s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.7494419642857143\nTrain loss: 0.10037846092520089\n","output_type":"stream"},{"name":"stderr","text":"Epoch:  27%|██▋       | 3/11 [00:33<01:29, 11.18s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.7276785714285714\nTrain loss: 0.07285717759152939\n","output_type":"stream"},{"name":"stderr","text":"Epoch:  36%|███▋      | 4/11 [00:44<01:18, 11.18s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.7020089285714286\nTrain loss: 0.06299123026687524\n","output_type":"stream"},{"name":"stderr","text":"Epoch:  45%|████▌     | 5/11 [00:55<01:07, 11.21s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.6886160714285714\nTrain loss: 0.06579592939594696\n","output_type":"stream"},{"name":"stderr","text":"Epoch:  55%|█████▍    | 6/11 [01:07<00:55, 11.20s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.6707589285714286\nTrain loss: 0.06807969876661382\n","output_type":"stream"},{"name":"stderr","text":"Epoch:  64%|██████▎   | 7/11 [01:18<00:44, 11.18s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.6707589285714286\nTrain loss: 0.06934677202511451\n","output_type":"stream"},{"name":"stderr","text":"Epoch:  73%|███████▎  | 8/11 [01:29<00:33, 11.18s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.7042410714285714\nTrain loss: 0.06060597123899337\n","output_type":"stream"},{"name":"stderr","text":"Epoch:  82%|████████▏ | 9/11 [01:40<00:22, 11.16s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.7260044642857143\nTrain loss: 0.04914694189511497\n","output_type":"stream"},{"name":"stderr","text":"Epoch:  91%|█████████ | 10/11 [01:51<00:11, 11.22s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.7516741071428572\nTrain loss: 0.06967116060570397\n","output_type":"stream"},{"name":"stderr","text":"Epoch: 100%|██████████| 11/11 [02:03<00:00, 11.18s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.7198660714285714\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Prepare testset with the same preprocessing","metadata":{"id":"4onlufXWCqWi"}},{"cell_type":"code","source":"#============= Read CSV and apply data preperation =============#\ndf_submit = pd.read_csv(\"./Twitter_test.csv\")\n\ndf_submit[\"tweet\"] = df_submit.tweet.apply(lambda x: data_cleaning(x))\ndf_submit=data_preprocessing(df_submit)\n\ndf_submit['tweet']=df_submit['tweet'].apply(lambda x: arabert_prep.preprocess(x))\n\n# Tokenize the sentences using bert tokenizer\ndf_submit[\"bert_tokens\"] = df_submit.tweet.apply(lambda x: tokenizer(x).tokens())","metadata":{"pycharm":{"name":"#%%\n"},"id":"yJ2XSmwSugw6","outputId":"e867b33c-053f-44b5-ecbf-c6560935b350","execution":{"iopub.status.busy":"2022-09-20T06:51:14.120216Z","iopub.execute_input":"2022-09-20T06:51:14.120750Z","iopub.status.idle":"2022-09-20T06:51:14.642756Z","shell.execute_reply.started":"2022-09-20T06:51:14.120700Z","shell.execute_reply":"2022-09-20T06:51:14.641421Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:20: DeprecationWarning: 'emoji.get_emoji_regexp()' is deprecated and will be removed in version 2.0.0. If you want to remove emoji from a string, consider the method emoji.replace_emoji(str, replace='').\nTo hide this warning, pin/downgrade the package to 'emoji~=1.6.3'\n","output_type":"stream"}]},{"cell_type":"code","source":"bert_tokens_submit = df_submit[\"bert_tokens\"]","metadata":{"id":"G7iPpop4ytzT","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-09-20T06:51:16.123068Z","iopub.execute_input":"2022-09-20T06:51:16.123556Z","iopub.status.idle":"2022-09-20T06:51:16.130422Z","shell.execute_reply.started":"2022-09-20T06:51:16.123522Z","shell.execute_reply":"2022-09-20T06:51:16.128929Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n# In the original paper, the authors used a length of 512.\nMAX_LEN = 64\n# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\ninput_ids_submit = [tokenizer.convert_tokens_to_ids(x) for x in bert_tokens_submit]\n# Pad our input tokens\ninput_ids_submit = pad_sequences(input_ids_submit, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n# Create attention masks\nattention_masks_submit = []\n\n# Create a mask of 1s for each token followed by 0s for padding\nfor seq in input_ids_submit:\n    seq_mask = [float(i>0) for i in seq]\n    attention_masks_submit.append(seq_mask)","metadata":{"id":"Z1vJgu5f0A-3","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-09-20T06:51:35.743857Z","iopub.execute_input":"2022-09-20T06:51:35.744314Z","iopub.status.idle":"2022-09-20T06:51:35.791003Z","shell.execute_reply.started":"2022-09-20T06:51:35.744283Z","shell.execute_reply":"2022-09-20T06:51:35.789768Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Convert all of our data into torch tensors, the required datatype for our model\ninputs_submit = torch.tensor(input_ids_submit)\nmasks_submit = torch.tensor(attention_masks_submit)\n\n# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n# with an iterator the entire dataset does not need to be loaded into memory\nbatch_size = 64\nsubmit_data = TensorDataset(inputs_submit, masks_submit)\n\n# do not use shuffle, we need the preds to be in same order\nsubmit_dataloader = DataLoader(submit_data, batch_size=batch_size)#, shuffle=True)","metadata":{"id":"HUeIG5WD0M9n","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-09-20T06:51:38.372975Z","iopub.execute_input":"2022-09-20T06:51:38.374034Z","iopub.status.idle":"2022-09-20T06:51:38.385581Z","shell.execute_reply.started":"2022-09-20T06:51:38.373986Z","shell.execute_reply":"2022-09-20T06:51:38.383687Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Put the model in an evaluation state\nmodel.eval()\n\n# Transfer model to GPU\nmodel.to(\"cuda\")\n\noutputs = []\nfor input, masks in submit_dataloader:\n    torch.cuda.empty_cache() # empty the gpu memory\n\n    # Transfer the batch to gpu\n    input = input.to('cuda')\n    masks = masks.to('cuda')\n\n    # Run inference on the batch\n    output = model(input, attention_mask=masks)[\"logits\"]\n\n    # Transfer the output to CPU again and convert to numpy\n    output = output.cpu().detach().numpy()\n\n    # Store the output in a list\n    outputs.append(output)\n\n# Concatenate all the lists within the list into one list\noutputs = [x for y in outputs for x in y]\n\n# Inverse transform the label encoding\npred_flat = np.argmax(outputs, axis=1).flatten()\noutput_labels = lable_encoder.inverse_transform(pred_flat)","metadata":{"id":"K-32QySM0Qpu","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-09-20T06:51:39.943560Z","iopub.execute_input":"2022-09-20T06:51:39.944032Z","iopub.status.idle":"2022-09-20T06:51:42.491019Z","shell.execute_reply.started":"2022-09-20T06:51:39.943998Z","shell.execute_reply":"2022-09-20T06:51:42.489488Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({\"Id\":np.arange(1, len(output_labels)+1), \"class\":output_labels})\n# save (submission)\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"id":"Az3ccyKv0SYu","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-09-20T06:53:59.822370Z","iopub.execute_input":"2022-09-20T06:53:59.822828Z","iopub.status.idle":"2022-09-20T06:53:59.837038Z","shell.execute_reply.started":"2022-09-20T06:53:59.822794Z","shell.execute_reply":"2022-09-20T06:53:59.835641Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"NAo2Q_2F06Je","pycharm":{"name":"#%%\n"}},"execution_count":null,"outputs":[]}]}